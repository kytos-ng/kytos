:EP: 38
:Title: Broadcasting events
:Authors:
	- Luis;
	- Italo;
	- Vinicius;
	- Jeronimo Bezerra jbezerra@fiu.edu;
:Issued Date: to be defined
:Status: Pending
:Type: Standards Track

*****************************************
EP03X - Broadcasting events
*****************************************

########
Abstract
########

This blueprint details how Kytos should export/broadcast its events to external applications safely and scalable. 

##########
Motivation
##########

With Kytos running in production, we learned that Kytos events can be leveraged for multiple purposes, from capacity planning to topology evaluation, from troubleshooting to accounting. However, creating Kytos napps to only monitor the environment should be avoided to minimize possible impacts on running the kytos instance. 
Moreover, this NApp can expedite the prototyping of new applications and frameworks that need an understanding the network topology and control plane, such as the AtlanticWave-SDX project. 
Based on the information of the AtlanticWave-SDX project, the system design integrates the Kytos-ng SDN platform and the SDX Local Controller SDX-LC. 
This integration aims to fulfill the functionalities required by the AtlanticWave-SDX project. 

Here is an overview of the AtlanticWave-SDX project components:

:Kytos-ng Topology: 
Integrating with the Kytos-ng framework provides a convenient API for retrieving the OXPO topology. This design choice leverages Kytos-ng's capabilities and simplifies obtaining topology data from the SDN platform. To fetch the topology data, the SDX-LC CRON function can request GET or pull through the endpoint: 

"http://localhost:8181/api/kytos/topology/v3/".

:Kytos Event Handling: 
Kytos-ng uses Kytos Event decorated functions within the Kytos-sdx-topology Napp to handle specific events related to topology changes. 
This event-driven approach enables the system to respond promptly and capture changes in the Kytos topology. It also allows for flexibility in managing different types of events and provides a scalable solution.

:Kytos-sdx-topology Napp: 
The Kytos-sdx-topology Napp should actively monitor topology changes by subscribing to events using the Kytos Event decorated function. This design choice must ensure the system stays updated with any modifications in the Kytos-ng topology and enables real-time processing of topology-related events.

Kytos-sdx-topology kytos Napp
https://github.com/atlanticwave-sdx/kytos-sdx-topology

Actual kytos-sdx-topology napp is designed to do the whole process in one napp, In atlanticwave sdx infrastructure, it's crucial to monitor the status of interdomain links for connectivity and reliability. This use case involves setting up a system to detect link status changes (up/down) and notify consumers about these events. 

The purpose of this blueprint is to design and evaluate the introduction of a message queue and separate the process as follows:

Creating two NApps to separate the event listener and treatment from the post office message broker simplifies the system architecture, enhances modularity, and improves fault tolerance. Here's how:

Separating the message broker interaction into two distinct NApps makes the system architecture more modular and easier to manage. We can develop, deploy, and scale each NApp independently, providing flexibility and adaptability. Additionally, isolating the message broker functionality enhances fault tolerance. If one NApp experiences issues or failures, it does not directly impact the other NApp or the message broker. This isolation improves the overall robustness and reliability of the system.

#######################
Simplified Architecture
#######################

The overall architecture becomes more straightforward and understandable by dividing the system into two distinct NApps, each with specific responsibilities. The event listener NApp focuses solely on capturing and processing events, while the post office message broker NApp handles message queuing and delivery. This clear separation of concerns makes it easier to reason about and maintain the system.

###################
Enhanced Modularity
###################

Separating the event listener and message broker functionalities into separate NApps promotes modularity within the system. We can develop, deploy, and scale each NApp independently, facilitating more manageable maintenance and updates without affecting other system components. Additionally, one NApp can implement new features or enhancements without affecting the other, increasing overall flexibility.

########################
Improved fault tolerance
########################

Isolating the event listener and message broker functionalities improves fault tolerance. If one NApp experiences issues or failures, it is less likely to impact the other NApp or the overall system. This isolation helps prevent cascading failures and ensures critical functionalities, such as event processing and message delivery, remain operational despite individual component failures.

######################################
Streamlined Development and Deployment
######################################

Having separate NApps dedicated to event handling and message queuing simplifies the development and deployment processes. Developers can focus on specific tasks within each NApp, leading to faster development cycles and easier testing. Each NApp can deploy independently without relying on other components, which simplifies the deployment process.

1. Design and implement a listen-events app capable of listening to events, discriminating against them, and sending them to a third-party post office that can handle, queue, and deliver messages. The listen-events Napp distinguishes between various events generated within the Kytos network infrastructure. It identifies different event categories based on predefined criteria associated with each event.

******************
listen_events NApp
******************

The listen_events NApp is responsible for two essential functions:

##########################################
Publishing KytosEvents to a Message Broker
##########################################

Design and implement a Napp capable of listening to events, discriminating them, and sending them to a third-party message broker for processing.

Utilize the provided listen_event function to distinguish between various event types based on predefined criteria.

Here is the actual listen event function

\@listen\_to\(
            "kytos/topology.link\_\*",
            "kytos/topology.switch.\*",
            pool="dynamic_single"\)
def listen_event(self, event=None):
        """Function meant for listen topology"""

        :if event is not None and self.version_control:

            dpid = ""

            :if event.name in settings.ADMIN_EVENTS:

                switch_event = {

                        "version/control.initialize": True,

                        "kytos/topology.switch.enabled": True,

                        "kytos/topology.switch.disabled": True

                        }

	    	:if switch_event.get(event.name, False):
                    event_type = "administrative"
                    dpid = event.content["dpid"]
                :else:
                    event_type = None
            elif event.name in settings.OPERATIONAL_EVENTS and \
                    event.timestamp is not None:
                event_type = "operational"
            else:
                event_type = None

            if event_type is None:
                return {"event": "not action event"}

This napp must use async.io for all asynchronous calls, for instance, \`\@alisten\_to\` instead of \`\@listen\_to\`

Setting file has the following events

:ADMIN_EVENTS = [:
        "version/control.initialize",

        "kytos/topology.switch.enabled",

	"kytos/topology.switch.disabled",

        "kytos/topology.switch.metadata.added",

        "kytos/topology.interface.metadata.added",

        "kytos/topology.link.metadata.added",

        "kytos/topology.switch.metadata.removed",

        "kytos/topology.interface.metadata.removed",

        "kytos/topology.link.metadata.removed",

        ]

:OPERATIONAL_EVENTS = [:
        "topology_loaded",

        "kytos/topology.link_up",

        "kytos/topology.link_down",

]

Leverage wildcards in topic exchange routing to route messages based on specific event patterns. For example:

"kytos\/topology.\*" to capture all topology-related events.

"kytos\/core.\*" to capture all core-related events.

Ensure message continuity by only publishing messages if the consumer has consumed the previous message or if there is a status change.

#####################################
Filtering KytosEvents for Publication
#####################################

This NApp is responsible for filtering KytosEvents before publishing them to the message broker.

Filtering may involve selecting specific types of events, applying rules or criteria to determine which events are relevant for publication, or transforming events into a different format as downstream consumers require.

The NApp optimizes communication by only transmitting relevant data to the message broker via filtering events, reducing unnecessary network traffic and processing overhead.

Overall, this NApp acts as a bridge between Kytos' internal message and the message broker, facilitating the exchange of events between the internal network infrastructure and external systems. It guarantees filtering and reliable transmission of relevant events to the message broker for downstream applications or service consumption. 

The Broadcasting Events Napp (listen_events napp) must be able to utilize any replaceable message transportation system.

This requirement underscores the necessity for flexibility and adaptability in the system's architecture. Therefore, separate the Broadcasting listen_events Napp and the Events Consumer Napp from the Message Queue Napp.
The listen_events NApp detects topology changes in real time disregards historical data, and may not require a cluster connection. In this scenario, we should design the NApp to manage real-time events effectively, eliminating the need for persistent storage or cluster connectivity and delivering a robust, high-performing solution.

If the message broker isn't available but the NApp needs to publish a message, it should implement an alarm log system to report the problem. Designed for connectionless message delivery, this mechanism regularly reconnects to the broker and retries publishing the message once the connection reestablishes. We carefully choose the retry interval, a configurable setting, to balance the need for timely message delivery and minimize unnecessary network traffic.

##################################################
Handling of events when the broker isn't available
##################################################

###############
Retry Mechanism
###############

The NApp should implement a retry mechanism for publishing messages when the broker isn't available, following a predefined retry strategy. This ensures the eventual delivery of messages once the broker regains accessibility.

Restarting from the Real-Time Point: In the event of a restart, the NApp should efficiently restart and resume from the real-time point, as the events relate to real-time topology changes and historical data is irrelevant. It should promptly discard any buffered or stored events related to previous topology changes and begin processing new events as they occur, ensuring optimal performance.

We should design the listen_events NApp to restart and resume from the current real-time point in case of a restart or failure, as historical data is irrelevant. Upon restart, the NApp should discard any buffered or cached data related to previous events and begin processing new events as they occur. This approach keeps the NApp's focus on real-time data processing, free from unnecessary historical data.

This approach enhances application scalability and fault tolerance by reducing dependencies between interconnected systems. Additionally, it facilitates better handling of system failures or temporary unavailability, thereby strengthening the overall robustness of the architecture.


2.- Message queues decouple components within the system. listen_events Napp can transmit updates without the continuous availability of consumers, like SDX Napp or BAPM applications. Moreover, the persistent nature of message queues ensures that if any application experiences a restart, it can seamlessly resume processing messages from its designated queue once it is back online.

The "listen_events" Napp will communicate with the topic exchange within the "mq_producer" Napp. 

****************
mq_producer NApp
****************

The mq_producer  NApp should support parameterizing the exchange and routing key for publishing a given set of events. This capability allows for precise control over the distribution of events in the message broker's infrastructure.

This topic exchange operates similarly to a direct exchange but introduces a more adaptable routing mechanism based on routing patterns. Unlike direct exchanges, which rely on fixed routing keys, topic exchanges utilize wildcards for message routing, enhancing flexibility.

Here's a breakdown of its operation:

Instead of depending on specific routing keys, topic exchanges route messages by comparing a message's routing key with predefined patterns.

This comparison determines the routing of messages to one or more queues based on their correspondence with the specified patterns.

The routing key consists of a series of words separated by periods (".").

In summary, the topic exchange facilitates nuanced and dynamic message routing through wildcard patterns, offering increased flexibility and versatility in distributing messages within the messaging system.

######################
Exchange Configuration 
######################

An exchange is a routing mechanism that receives messages from producers and routes them to queues based on routing rules. 
The mq_producer NApp enables the exchange specification to publish the events.

########################
Exchange Existence Check
########################

At setup() method of mq_producer NApp should verify whether the target exchange exists within the message broker.
This check ensures that only legitimate exchanges receive messages, avoiding potential errors from non-existent exchanges.

#################################
Exchange Creation if Non-existent
#################################

If the exchange does not exist, the mq_producer NApp should include logic to create the exchange dynamically.
Even if the exchange wasn't previously defined, this dynamic creation ensures it's ready for message publishing.

####################################
Error Handling for Exchange Creation
####################################

We should implement proper error handling to address scenarios where exchange creation fails.
The NApp should handle exceptions gracefully and potentially log relevant error messages for troubleshooting purposes.
By incorporating these specifications, the mq_producer NApp can manage the target exchange effectively, ensuring reliable message publishing and providing resilience in dynamic exchange creation.		

The mq_producer Napp can tailor the message routing behavior by parameterizing the exchange to suit the application's needs.

#########################
Routing Key Specification
#########################

The mq_producer NApp supports parameterizing the routing keys associated with each set of events. The message broker uses a routing key as a message attribute to route messages to the appropriate queues.

The mq_producer Napp defines custom routing keys based on event characteristics or metadata, allowing targeted messages to be delivered to specific queues.
This NApp offers enhanced flexibility and is configurable in event publishing workflows by supporting the parameterization of exchange and routing key settings. It tailors message routing behavior to align with specific use cases and optimizes message delivery and consumption within the message broker's ecosystem.

This code will generate routing keys and topic exchanges based on the event names and content. The generate_routing_key function generates routing keys based on the event type and name, while the generate_topic_exchange function generates topic exchanges based on the event type. You can adjust the logic as needed to fit your specific requirements and event structures.

# Define function to generate routing key based on event
:def generate_routing_key(event):
    event_type = None
    :if event.name in settings.ADMIN_EVENTS:
        switch_event = {
            "version/control.initialize": True,
            "kytos/topology.switch.enabled": True,
            "kytos/topology.switch.disabled": True
        }
        if switch_event.get(event.name, False):
            event_type = "administrative"
            dpid = event.content["dpid"]
        else:
            event_type = None
    :elif event.name in settings.OPERATIONAL_EVENTS and event.timestamp is not None:
        event_type = "operational"
    :else:
        event_type = None

    :if event_type:
        return f"{event_type}.{event.name}"
    :else:
        return None

# Define function to generate topic exchange based on event type
:def generate_topic_exchange(event):
    :if event.name in settings.ADMIN_EVENTS:
        return "admin_events"
    :elif event.name in settings.OPERATIONAL_EVENTS:
        return "operational_events"
    :else:
        return "other_events"

# Generate routing key and topic exchange for each event
:for event_name in settings.ADMIN_EVENTS + settings.OPERATIONAL_EVENTS:
    event = Event(event_name)  # Assuming Event is a class representing an event
    routing_key = generate_routing_key(event)
    topic_exchange = generate_topic_exchange(event)
    print(f"Event: {event.name}, Routing Key: {routing_key}, Topic Exchange: {topic_exchange}")


############################
Secret/Auth parameterization
############################

:Authorization through Environment Variables:

Implementing MQ authorization through environment variables is a common and practical approach. This approach enables the secure storage of credentials or authentication tokens beyond the codebase, thereby mitigating the risk of exposure.
During runtime, the NApp can retrieve these credentials from environment variables, ensuring the confidentiality of sensitive information.
By implementing authentication through environment variables and ensuring that the NApp can seamlessly restart and resume from the real-time point, it can effectively handle topology changes in real-time without the need for cluster connectivity or historical data persistence. This reliability feature ensures that the NApp never misses a beat, even in the face of unexpected events.

:import pika:

# RabbitMQ connection parameters

RABBITMQ_HOST = 'localhost'

RABBITMQ_PORT = 5672

RABBITMQ_USERNAME = 'guest'

RABBITMQ_PASSWORD = 'guest'

RABBITMQ_VIRTUAL_HOST = '/'

# Function to create a connection to RabbitMQ
:def create_rabbitmq_connection():
    credentials = pika.PlainCredentials(RABBITMQ_USERNAME, RABBITMQ_PASSWORD)
    parameters = pika.ConnectionParameters(RABBITMQ_HOST, RABBITMQ_PORT, RABBITMQ_VIRTUAL_HOST, credentials)
    connection = pika.BlockingConnection(parameters)
    return connection

# Function to create queues
:def create_queues(connection, event_names):
    channel = connection.channel()
    for event_name in event_names:
        queue_name = event_name.replace('.', '_')  # Replace dots in event name with underscores for queue name
        channel.queue_declare(queue=queue_name, durable=True)  # Declare a durable queue
        print(f"Queue '{queue_name}' created.")

#############
Example usage
#############

:if __name__ == "__main__":
    # Generate list of event names
    all_events = settings.ADMIN_EVENTS + settings.OPERATIONAL_EVENTS
    
    # Create RabbitMQ connection
    connection = create_rabbitmq_connection()

    # Create queues
    create_queues(connection, all_events)

    # Close RabbitMQ connection
    connection.close()


**********
AMQP 0.9.1
**********

A highly efficient and versatile protocol empowers RabbitMQ, a widely acclaimed message broker, to communicate seamlessly across various systems. This robust combination ensures reliable communication between different components of a distributed system.

********
aio-pika
********

https://aio-pika.readthedocs.io/en/latest/rabbitmq-tutorial/1-introduction.html

By implementing future optional asynchronous I/O, we can significantly improve our system's efficiency. This powerful feature will allow us to simultaneously handle multiple input/output operations, resulting in a faster and more responsive system overall."

We will evaluate aio-pika an asynchronous AMQP client library designed for Python applications. It enables asynchronous and efficient interaction with RabbitMQ, making it well-suited for high-performance applications or systems that require non-blocking I/O operations.

By leveraging the power of aio-pika, we can seamlessly integrate RabbitMQ's robust messaging capabilities into our Kytos Napps. This potent combination of RabbitMQ's strength and aio-pika's asynchronous nature inspires the creation of scalable and responsive distributed systems, fueling our projects' potential.

In conclusion, isolating the Broadcasting Events Napp and the Events Consumer Napp into separate microservices from the Message Queue Producer and Consumer Napps promotes flexibility, modularity, scalability, and resilience in the system's architecture. This design approach enables the system to adapt to changing requirements and technologies while maintaining robustness and efficiency in event broadcasting and consumption.

********
Use Case 
********

###################################
Interdomain Link Up/Down Monitoring
###################################

:Scenario: 

In SDX, monitoring the status of interdomain links for connectivity and reliability is crucial. This use case involves setting up a system to detect link status changes (up/down) and notify consumers about these events through message queues.
Components:

:Producer: 

Generates events based on link status changes.
Consumer: Monitors link status by consuming messages from the appropriate queues.

:Implementation:

################################
Link Status Queue Initialization
################################

Each interdomain link has its dedicated queue.
Queues are either dynamically created or configured based on settings.

########
Benefits
########

Real-time monitoring of interdomain link status.

Scalable solution with dynamically created queues.

Fault-tolerant design ensures persistent handling of link-down events.

Flexibility in queue management allows dynamic addition or configuration based on settings.

With its efficient producer-consumer relationship, the system orchestrates message handling for specific link-down events. This design ensures smooth communication, preventing message flooding and instilling confidence in its performance. 

:Here's how it operates:

:Producer Advertises Link Down: 

When a link-down event occurs, the producer publishes a message indicating the link's status change to down.

Consumer's Crucial Role in Link Status Monitoring: 

The consumer, a key player, diligently monitors the message queue for link-down events. However, if the consumer still needs to read the message or the link status hasn't changed, the producer refrains from continuously writing messages of the same status to the queue.

Preventing Message Flooding: 

To prevent message flooding and conserve system resources, the producer only writes messages to the queue if the consumer has consumed the previous message or the link status has changed.

Message continuity is based on consumer activity:

If the consumer has yet to acknowledge or process the previous message, the producer waits for them to read it before publishing another message with the same status. In the same way, if the link status changes, the producer updates the message accordingly. 

Publisher confirmations play a vital role in ensuring the reliability of message delivery:

The mq_producer NApp gains confidence in successfully processing and routing a message upon receiving acknowledgment from the message broker, which prevents the mq_producer from continuously adding redundant messages to the queue. This acknowledgment mechanism ensures the reliable processing and routing of messages. 

In summary, while ensuring the existence of the target exchange and setting the mandatory bit when publishing messages are essential considerations for message routing, publisher confirmation adds an extra layer of assurance for message delivery reliability. 

Overall, this use case demonstrates how message queues can be effectively utilized for monitoring and managing interdomain link status changes in an SDX network infrastructure, ensuring timely detection and response to connectivity issues.
